{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d92df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp39-cp39-win_amd64.whl (13.7 MB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\n",
      "ERROR: No matching distribution found for sqlite3\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu sentence-transformers rank-bm25 tqdm nltk sqlite3 pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a93f675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a2afbb00f14abba7f20f46d0cef89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  58%|#####7    | 273M/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691405f70e554ab2bebc0c244f3751f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d3c3f5b5fa4e11ac2f26ba3a7804e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e0a566ba9648ed83844d5ea8201bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7106fee1494fbea796408cfa0c8e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 28125 text chunks from database.\n",
      "🔄 Generating embeddings for text chunks...\n",
      "✅ Generated 28125 embeddings of size 384.\n",
      "✅ FAISS index created and saved.\n",
      "🔄 Building BM25 index...\n",
      "✅ BM25 keyword search index created.\n",
      "\n",
      "🔍 Query Results:\n",
      "1. EARS ASSIS POST PREP SEL PSE SEL EAE தத SiLteegse Seep eee er pe PRIESTESS OS லய கதத SESS T ESTERS SS es eton terete lteter tires sever teass soe te tees a PPIT ELISE LOSES TNS ரதத par os தத LOSS SSIS...\n",
      "2. நப ம ய LEMAR பப பமப மமம அமவ ஸ றகள க த க பங she ந ந ககக acts ட ந பயககக [ற ன த a னனர TA லவவ கம ம i 2 பங ந ப வ ர a ள he vs a 3 ay 3 ne ca my x pula பமடம த ள oat ra bi ie eae ங ப Besson ய தய 4\" a rina eed...\n",
      "3. ஸர வரதபடடர ஆணட வக எஞச கட[வப] பதவ[கக 2. ர தரவடடர வககக ப]டரய வடததடனன ஊ 3. 1] மல கறனன மனகட வர இலரக 94 த. ந. ௮. தலலயலதற தடர எண 509 2004 மவடடம கனனயககமர ஆடச ஆணட Soo வடடம கலகளம வரலறற ஆணட கலலம 919 க.ப. 1744 ஊ...\n",
      "✅ FAISS, BM25, and text data saved.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Sentence Transformer Model for Tamil-English\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# ✅ Step 1: Load Preprocessed Text Chunks from SQLite\n",
    "def load_text_chunks():\n",
    "    \"\"\"Load all preprocessed text chunks from SQLite.\"\"\"\n",
    "    conn = sqlite3.connect(\"processed_texts_1.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT content FROM texts;\")\n",
    "    text_chunks = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = load_text_chunks()\n",
    "print(f\"✅ Loaded {len(text_chunks)} text chunks from database.\")\n",
    "\n",
    "# ✅ Step 2: Convert Text Chunks into Embeddings\n",
    "print(\"🔄 Generating embeddings for text chunks...\")\n",
    "embeddings = model.encode(text_chunks, convert_to_numpy=True)\n",
    "print(f\"✅ Generated {len(embeddings)} embeddings of size {embeddings.shape[1]}.\")\n",
    "\n",
    "# ✅ Step 3: Index Embeddings Using FAISS\n",
    "embedding_size = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index to disk\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "print(\"✅ FAISS index created and saved.\")\n",
    "\n",
    "# ✅ Step 4: Implement BM25 for Keyword Search\n",
    "print(\"🔄 Building BM25 index...\")\n",
    "tokenized_corpus = [word_tokenize(text.lower()) for text in text_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"✅ BM25 keyword search index created.\")\n",
    "\n",
    "# ✅ Step 5: Implement Hybrid Search (BM25 + FAISS)\n",
    "def hybrid_search(query, top_n=5):\n",
    "    \"\"\"Retrieve results using both FAISS and BM25 for best accuracy.\"\"\"\n",
    "    \n",
    "    # Convert query into embedding\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # FAISS semantic search\n",
    "    D, I = index.search(query_embedding, top_n)\n",
    "    faiss_results = [text_chunks[i] for i in I[0]]\n",
    "\n",
    "    # BM25 keyword search\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_top_indices = np.argsort(bm25_scores)[-top_n:][::-1]\n",
    "    bm25_results = [text_chunks[i] for i in bm25_top_indices]\n",
    "\n",
    "    # Merge results\n",
    "    combined_results = list(set(faiss_results + bm25_results))\n",
    "\n",
    "    return combined_results[:top_n]\n",
    "\n",
    "# ✅ Step 6: Test Retrieval System\n",
    "query = \"தமிழக தொல்லியல் ஆய்வுகள்\"  # Example Tamil query\n",
    "results = hybrid_search(query, top_n=3)\n",
    "\n",
    "print(\"\\n🔍 Query Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result[:200]}...\")  # Print first 200 characters\n",
    "\n",
    "# ✅ Step 7: Save FAISS & BM25 Models for Future Use\n",
    "with open(\"bm25_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bm25, f)\n",
    "\n",
    "with open(\"text_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_chunks, f)\n",
    "\n",
    "print(\"✅ FAISS, BM25, and text data saved.\")\n",
    "\n",
    "# ✅ Step 8: Function to Load Models in Future\n",
    "def load_models():\n",
    "    \"\"\"Load FAISS index, BM25 model, and text chunks.\"\"\"\n",
    "    index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "    with open(\"bm25_model.pkl\", \"rb\") as f:\n",
    "        bm25 = pickle.load(f)\n",
    "\n",
    "    with open(\"text_chunks.pkl\", \"rb\") as f:\n",
    "        text_chunks = pickle.load(f)\n",
    "\n",
    "    print(\"✅ FAISS & BM25 models loaded.\")\n",
    "    return index, bm25, text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42655217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.10.0-cp39-cp39-win_amd64.whl (13.7 MB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from faiss-cpu) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging->faiss-cpu) (3.0.4)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b007f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.27.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.3.15)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Installing collected packages: pyyaml, fsspec, huggingface-hub, tokenizers, safetensors, transformers, sentence-transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "Successfully installed fsspec-2025.3.0 huggingface-hub-0.29.3 pyyaml-6.0.2 safetensors-0.5.3 sentence-transformers-3.4.1 tokenizers-0.21.1 transformers-4.49.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "chatterbot-corpus 1.2.0 requires PyYAML<4.0,>=3.12, but you have pyyaml 6.0.2 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ad7eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25Note: you may need to restart the kernel to use updated packages.\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rank-bm25) (1.26.4)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install rank-bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f95b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading Sentence Transformer Model...\n",
      "✅ Loading text chunks from database...\n",
      "✅ Loaded 25184 text chunks from database.\n",
      "🔄 Generating embeddings for text chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4126f9480d9b4087b6ae196b8400356e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 25184 embeddings of size 384.\n",
      "🔄 Creating FAISS index...\n",
      "✅ FAISS index created and saved.\n",
      "🔄 Building BM25 index...\n",
      "✅ BM25 keyword search index created.\n",
      "✅ FAISS, BM25, and text data saved.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure required NLTK data is available\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# ✅ Load Pre-trained Multilingual Embedding Model\n",
    "print(\"🔄 Loading Sentence Transformer Model...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# ✅ Step 1: Load Text Data from SQLite\n",
    "def load_text_chunks(db_path=\"processed_texts_final_3.db\"):\n",
    "    \"\"\"Load preprocessed text chunks from SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT content FROM texts\")\n",
    "    text_chunks = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    return text_chunks\n",
    "\n",
    "print(\"✅ Loading text chunks from database...\")\n",
    "text_chunks = load_text_chunks()\n",
    "print(f\"✅ Loaded {len(text_chunks)} text chunks from database.\")\n",
    "\n",
    "# ✅ Step 2: Convert Text to Vector Embeddings\n",
    "print(\"🔄 Generating embeddings for text chunks...\")\n",
    "embeddings = model.encode(text_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "print(f\"✅ Generated {len(embeddings)} embeddings of size {embeddings.shape[1]}.\")\n",
    "\n",
    "# ✅ Step 3: Build FAISS Index\n",
    "print(\"🔄 Creating FAISS index...\")\n",
    "embedding_dim = embeddings.shape[1]  # Get the embedding dimension (384)\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 (Euclidean) distance-based index\n",
    "index.add(embeddings)  # Add embeddings to FAISS index\n",
    "faiss.write_index(index, \"faiss_index.bin\")  # Save FAISS index\n",
    "print(\"✅ FAISS index created and saved.\")\n",
    "\n",
    "# ✅ Step 4: Build BM25 Index for Keyword Search\n",
    "print(\"🔄 Building BM25 index...\")\n",
    "tokenized_corpus = [word_tokenize(text.lower()) for text in text_chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)  # Create BM25 index\n",
    "with open(\"bm25_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(bm25, f)  # Save BM25 model\n",
    "print(\"✅ BM25 keyword search index created.\")\n",
    "\n",
    "# ✅ Save Processed Text Chunks for Retrieval\n",
    "with open(\"text_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(text_chunks, f)\n",
    "\n",
    "print(\"✅ FAISS, BM25, and text data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d60bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File exists: faiss_index.bin\n",
      "📦 File size: 38682669 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "faiss_index_path = \"faiss_index.bin\"\n",
    "\n",
    "# Check if the file exists and its size\n",
    "if os.path.exists(faiss_index_path):\n",
    "    print(f\"✅ File exists: {faiss_index_path}\")\n",
    "    print(f\"📦 File size: {os.path.getsize(faiss_index_path)} bytes\")\n",
    "else:\n",
    "    print(f\"❌ FAISS index file not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fc960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index successfully loaded with 25184 vectors\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "faiss_index_path = \"faiss_index.bin\"\n",
    "\n",
    "try:\n",
    "    index = faiss.read_index(faiss_index_path)\n",
    "    print(f\"✅ FAISS index successfully loaded with {index.ntotal} vectors\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading FAISS index: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498fa91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top 5 retrieved indexes: [[14153  1186  1193  8366 18918]]\n",
      "🔢 Distances: [[117.71161  117.820435 118.11324  118.19989  118.267395]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Create a dummy query vector (replace with an actual embedding later)\n",
    "query_vector = np.random.rand(1, index.d).astype(np.float32)\n",
    "\n",
    "# Perform FAISS search\n",
    "D, I = index.search(query_vector, 5)  # Retrieve top 5 results\n",
    "print(f\"🔍 Top 5 retrieved indexes: {I}\")\n",
    "print(f\"🔢 Distances: {D}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb80675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Top 5 BM25 results: [8736 1238  907 5783 5798]\n",
      "🔢 BM25 Scores: [24.456559612229086, 19.928549824957127, 18.59563451309987, 18.51387805492081, 18.51387805492081]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load BM25 model\n",
    "with open(\"bm25_index.pkl\", \"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "\n",
    "# Example query\n",
    "query_text = \"ancient Tamil inscriptions on Stone Pillar\"\n",
    "tokenized_query = query_text.lower().split()\n",
    "\n",
    "# Perform BM25 search\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "top_bm25_results = np.argsort(bm25_scores)[::-1][:5]\n",
    "\n",
    "print(f\"🔍 Top 5 BM25 results: {top_bm25_results}\")\n",
    "print(f\"🔢 BM25 Scores: {[bm25_scores[i] for i in top_bm25_results]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac5bf157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Hybrid Search Top Results: [8736, 1238, 907, 5783, 5798]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hybrid_search(query_embedding, query_text, faiss_weight=0.7, bm25_weight=0.3):\n",
    "    \"\"\"Combine FAISS (semantic) + BM25 (keyword) search\"\"\"\n",
    "\n",
    "    # FAISS search\n",
    "    D, faiss_results = index.search(query_embedding, 5)\n",
    "    \n",
    "    # BM25 search\n",
    "    tokenized_query = query_text.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    bm25_results = np.argsort(bm25_scores)[::-1][:5]\n",
    "\n",
    "    # Combine results (weighted sum)\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    for i, idx in enumerate(faiss_results[0]):\n",
    "        hybrid_scores[idx] = faiss_weight * (1 / (D[0][i] + 1e-5))  # Avoid division by zero\n",
    "\n",
    "    for i, idx in enumerate(bm25_results):\n",
    "        if idx in hybrid_scores:\n",
    "            hybrid_scores[idx] += bm25_weight * bm25_scores[idx]\n",
    "        else:\n",
    "            hybrid_scores[idx] = bm25_weight * bm25_scores[idx]\n",
    "\n",
    "    # Sort by final scores\n",
    "    sorted_results = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [r[0] for r in sorted_results[:5]]\n",
    "\n",
    "# Example hybrid search\n",
    "query_vector = np.random.rand(1, index.d).astype(np.float32)  # Replace with real query embedding\n",
    "query_text = \"ancient Tamil inscriptions on Stone Pillar\"\n",
    "\n",
    "top_hybrid_results = hybrid_search(query_vector, query_text)\n",
    "print(f\"🔍 Hybrid Search Top Results: {top_hybrid_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b917a13d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load FAISS, BM25, and Text Chunks\n",
    "with open(\"bm25_index.pkl\", \"rb\") as f:\n",
    "    bm25 = pickle.load(f)\n",
    "\n",
    "with open(\"text_chunks.pkl\", \"rb\") as f:\n",
    "    text_chunks = pickle.load(f)\n",
    "\n",
    "faiss_index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Load SentenceTransformer (MiniLM multilingual)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Load Flan-T5 for reranking and summarization (base, good on CPU)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", resume_download=True)\n",
    "\n",
    "# Hybrid Retriever (FAISS + BM25)\n",
    "def hybrid_search(query_text, top_k=10, faiss_weight=0.7, bm25_weight=0.3):\n",
    "    query_embedding = embedding_model.encode([query_text], convert_to_numpy=True)\n",
    "\n",
    "    # FAISS Search\n",
    "    D, faiss_indices = faiss_index.search(query_embedding, top_k)\n",
    "    faiss_scores = 1 / (D[0] + 1e-5)  # Similarity from L2 distance\n",
    "\n",
    "    # BM25 Search\n",
    "    tokenized_query = word_tokenize(query_text.lower())\n",
    "    bm25_scores_all = bm25.get_scores(tokenized_query)\n",
    "    bm25_top_indices = np.argsort(bm25_scores_all)[::-1][:top_k]\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores = {}\n",
    "    for i, idx in enumerate(faiss_indices[0]):\n",
    "        hybrid_scores[idx] = faiss_weight * faiss_scores[i]\n",
    "\n",
    "    for idx in bm25_top_indices:\n",
    "        hybrid_scores[idx] = hybrid_scores.get(idx, 0) + bm25_weight * bm25_scores_all[idx]\n",
    "\n",
    "    # Sort and return top-k\n",
    "    top_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [(idx, text_chunks[idx]) for idx, _ in top_indices[:top_k]]\n",
    "\n",
    "# Rerank Results using Flan-T5\n",
    "def rerank_with_t5(query, candidates, top_n=5):\n",
    "    rerank_inputs = [\n",
    "        f\"Query: {query}\\nPassage: {text}\" for _, text in candidates\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    for input_text in rerank_inputs:\n",
    "        inputs = t5_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
    "        with torch.no_grad():\n",
    "            outputs = t5_model.generate(**inputs, max_length=5)\n",
    "        score_text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        score = 1 if \"yes\" in score_text.lower() else 0\n",
    "        scores.append(score)\n",
    "\n",
    "    # Sort candidates by scores\n",
    "    reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [text for (idx, text), _ in reranked[:top_n]]\n",
    "\n",
    "# Summarize Top Passages\n",
    "def summarize_passages(passages):\n",
    "    combined_text = \" \".join(passages)\n",
    "    input_text = f\"summarize: {combined_text}\"\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = t5_model.generate(**inputs, max_length=100)\n",
    "    \n",
    "    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Run End-to-End Search with Summary\n",
    "def run_search(query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "\n",
    "    hybrid_results = hybrid_search(query, top_k=10)\n",
    "    reranked = rerank_with_t5(query, hybrid_results, top_n=3)\n",
    "\n",
    "    for i, passage in enumerate(reranked, 1):\n",
    "        print(f\"{i}. {passage}\\n\")\n",
    "\n",
    "    summary = summarize_passages(reranked)\n",
    "    print(f\"Summary:\\n{summary}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7130373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Query: தமிழகத்தில் உள்ள பழமையான கல்வெட்டுகள்\n",
      "\n",
      "1. . தமிழகத்தில் மட்டும் சுமார் 25,000 கல்வெட்டுகள் . வரலாற்றுக்கு தேவையான அடிப்படைச் சான்றுகளை இக்கல்வெட்டுகளின் அருமை காரணமாக அவற்றைப்படிக்க வேண்டி \" கல்வெட்டு முனைப்புத் திட்டம் ' சிறப்புத்திட்டத்தின் கீழ் 2004 செப்டம்பர் மாதம் தொடங்கி கல்வெட்டுகள் படியெடுக்கப்பட்டு வருகின்றன . இக்கல்வெட்டு முனைப்புத் திட்டத்தின் வாயிலாக ஆகஸ்ட் 2009 வரை 14,531 கலீவெட்டுகளும் ஏற்கனவே படியெடுத்த 7,833 கல்வெட்டுகளும் ஆக மொத்தம் 22,364 கல்வெட்டுகளும் இதுவரை இத்துறையில் படியெடுக்கப்பட்டுள்ளன . இத்துறையின் ஓய்வு பெற்ற இயக்குநர்கள் திரு . நடன . காசிநாதன் மற்றும் திரு . கு . தாமோதரன் செப்பேடுகள் , பிராமி கல்வெட்டுகள் நடுகல் கல்வெட்டுகள் , பாடல் கல்வெட்டுகள் தமிழ் கல்வெட்டுகளில் முக்கியமானவற்றைத் தொகுத்து உரிய விளக்கவுரையுடன் சிறப்புற எழுதியுள்ளனர் . , \" கல்வெட்டு அறிமுகம் . இந்நூல் வரலாற்று ஆய்வு மாணவர்கள் வரலாற்று ஆர்வலர்களிடையே உள்ள வரவேற்புக்கிணங்க மறுபதிப்பு சிறப்புடன்\n",
      "\n",
      "2. ஊன்றியுமுன்ளனர் . வலக்கை தர்சினி முத்திரையிலும் , இடக்கை கதையின் மீதும் . இதன் காலம் கி . பி . 8 - 10ம் நூற்றாண்டு . கருவறை - அர்த்த மண்டபத்தின் காலம் கருவறையிலும் , அர்த்த மண்டபத்திலும் உள்ள பழமையான கல்வெட்டுகள் முதல இராசராசனின் கல்வெட்டுகளாகும் . ஆனால அக்கல்வெட்டுகளில் கோயிலுக்குத் தானம் கொடுத்த செய்திகன்தாம் . இக்கோயிலை எடுப்பித்ததாக கல்வெட்டு இல்லை . பராந்தகன் காலத்துக் கல்வெட்டுகள் 13 , கோபுரத்திலும் தரையிலும் துண்டுக் கல்வெட்டுகளாகச் சிதறிக் கிடக்கின்றன . அக்கல்வெட்டுகள் வாளையூர் திருத்தான்தோன்றியாற்குத் தானம் கொடுத்ததைப் பற்றிக் கூறுகின்றன . முன்றாம் கிருஷ்ணன் கல்வெட்டும் வாளைஞர் வீதிவிடங்கற்கு நொந்தா விளக்கு தானம் கொடுத்ததைப் பற்றிக் கூறுகிறது . வரல் வல மகனால் ணணஙலால் சாண rg eg vee ஏக்க ee அரண ஓ னு ௨௬௯ வ en பராந்தகன் காலத்தில் அர்த்த\n",
      "\n",
      "3. 600 008 . அச்சிட்டோர் தரமணி மகளிர் கூட்டூறவு அச்சகம் . இராயப்பேட்டை . சென்னை - 600 014 . அட்டைப்படம் இசைக்கல்வெட்டு அரச்சலூர் , ஈரோடு மாவட்டம் . NJ முனைவர் தி . ப்ரீ . ஸ்ரீதர் , இஆப , முதன்மைச் செயலாளர் ( ம ) ஆணையர் தொல்லியல் துறை , தமிழ் வளர்ச்சி வளாகம் , ஆல்சு சாலை , சென்னை - 600 008 14 - 10 - 09 பதிப்புரை தமிழகத்தில் காலத்தைக் கடந்து ஆயிரக்கணக்கான அளவில் உள்ள கோயில்கள் , அவற்றில் பொறிக்கப்பட்டுள்ள கல்வெட்டுகள் ஒவ்வொன்றும் வரலாற்றுக்கு உதவும் சான்றுகளாகும் . வரலாற்று ஆதாரமாக தமிழ் பிராமி கல்வெட்டுகள் தொடங்கி அண்மைக் காலம் . கிடைக்கக் கூடிய கல்வெட்டுகளின் எண்ணிக்கை ஏராளம் . இந்தியாவிலேயே அதிகளவிலான கல்வெட்டு , செப்பேடு , ஓலைச்சுவடிகள் சான்றுகள் தழிழகத்தில் கிடைக்கின்றன\n",
      "\n",
      "📌 Summary:\n",
      "\n",
      "🔍 Query: Ancient inscriptions in Tamil Nadu\n",
      "\n",
      "1. production centres ancient tamil country scholars researchers professors students history heritage enthusiasts participated significantly year 50 inscriptions copied temples hills districts pudukottai thiruvannamalai districts tamil nadu innovation initiatives tanii schemes 324 tamil nadu innovation initiatives scheme year total sum rs47 lakh sanctioned three projects three publication tamil nadu inscriptions vol xiii ten selected monographs students completed remaining two projects creation virtual museum installation 131 floor projection mapping rajarajan site museum thanjavur nearing completion multidisciplinary approach 325 recognised valuable contribution specialised disciplines deeper analysis archaeological findings decided collaborate experts fields archaeobotany molecular biology population genetics environmental archaeology linguistic archaeology conclusion 326\n",
      "\n",
      "2. tourism culture religious endowments department art culture museums archaeology policy note demand 29 k pandiarajan minister tamil official language tamil culture government tamil nadu 2019 index sl contents page 1 art culture 186 2 museums 3 archaeology 107 department archaeology policy note introduction 3 archaeology experimental discipline concerned recovery systematic description methodical analytical study identification interpretation remains ancient humans obtain complete picture ancient culture well society possible extent objectives department 31 state department archaeology established 1961 following objectives identify conserve preserve ancient historical monuments 108 conduct systematic archaeological explorations excavations historically important sites throughout state documenting stone inscriptions copying deciphering\n",
      "\n",
      "3. journal epigraphical society india vol 23 1997 14 paneerselvam r important brahmi tamil inscriptions reconstruction geneology chera kings seminar tamil studies malaysia 1968 15 sesha lyear kg cera kings sangam period london luzac co 1937 16 shanmugam inscribed sherds graffiti excavation uraiyur ed kv raman 17 sridhar ts excavations archaeological sites tamil nadu state department archaeology 2005 18 sridhar 18 alagamkulam roman port city tamil nadu state department archaeology 2005 ஓ conclusion scholars different opinion capital sangam age chera king whether kerala present karur tamilnadu occuranceof coins inscribed potsherds excavation reveals kurur ancient city trade centre epigraphical archaeological numismatic evidences\n",
      "\n",
      "📌 Summary:\n",
      "tamil nadu inscriptions tanii schemes 324 tamil nadu innovation initiatives scheme year 50 inscriptions copied temples hills districts pudukottai thiruvannamalai districts tamil nadu innovation initiatives scheme year total sum rs47 lakh sanctioned three projects three publication tamil nadu inscriptions vol xiii ten\n"
     ]
    }
   ],
   "source": [
    "run_search(\"தமிழகத்தில் உள்ள பழமையான கல்வெட்டுகள்\")\n",
    "run_search(\"Ancient inscriptions in Tamil Nadu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a4c0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef84c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
